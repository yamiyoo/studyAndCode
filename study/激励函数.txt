激励函数
	不能用线性方程概述
	linear nonlinear
	y = Wx (W参数 x输入值 y参数)
	y = AF(Wx) (AF激励参数)
	激励函数必须可以微分

	默认首选激励函数
		卷积神经网络 relu 
		循环神经网络 tanh 或者是 relu

TensorFlow
训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来.

Tensor 张量意义 

张量（Tensor):

张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 [1]
一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]
二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]]
以此类推, 还有 三阶 三维的 …

例一  结果:训练参数
# 初始化所有之前定义的Variable

sess = tf.Session()
sess.run(init)          # Very important
#Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 
#每一次 training 的数据. 逐步提升神经网络的预测准确性.

for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(Weights), sess.run(biases))

#在TensorFlow的世界里，变量的定义和初始化是分开的，
#所有关于图变量的赋值和计算都要通过tf.Session的run来进行。
#想要将所有图变量进行集体初始化时
#应该使用tf.global_variables_initializer

session会话控制

variable
定义了某字符串是变量，它才是变量，这一点是与 Python 所不同的
	定义语法： state = tf.Variable()










